{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e802e046",
   "metadata": {},
   "source": [
    "# Deep Learning Hyperparameters & models for Regression Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53db917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc18c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(r'dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d275ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "# MinMaxScaler / StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(traindata)\n",
    "scaled_train = scaler.transform(traindata)\n",
    "scaled_test = scaler.transform(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(traindata)\n",
    "scaled_train = scaler.transform(traindata)\n",
    "scaled_test = scaler.transform(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8530c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]  #all columns as features\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb80298",
   "metadata": {},
   "source": [
    "### Sequential models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4974391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple dl model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, kernel_initializer='he_normal', activation='relu', input_shape=(n_features,)))\n",
    "model.add(Dense(1, kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d2919",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "**says about all the ways that can be used for the optimization of our neural networks.**\n",
    "\n",
    "\n",
    "##### Important Parameters to be considered while tuning a Neural Network\n",
    "- Number of Layers in a Neural Network\n",
    "- The Dropout Value\n",
    "- Learning Rate of the Neural Network\n",
    "- The Batch Size\n",
    "\n",
    "###### Choosing the Right Number of Hidden Layers in a Neural Network   \n",
    "- Choose 3 to 5 Hidden Layers for a Medium-Sized Dataset.\n",
    "- Neural Networks Might Suffer from underfitting if the hidden layers are less than 2.\n",
    "- Similarly, the Neural Networks can suffer from overfitting if the Number of Hidden layers is too high.\n",
    "- For Bigger Problems, Slowly increase the Hidden Layers and Check to see if Increasing Hidden Layers optimizes the Neural Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9afa5",
   "metadata": {},
   "source": [
    "##### Choosing the right Activation Function\n",
    "----- 'relu' activation function is used for regression problems\n",
    "\n",
    "- Sigmoid functions and their combinations generally work better in the case of classifiers\n",
    "- Sigmoid and tanh functions are sometimes avoided due to the vanishing gradient problem\n",
    "- ReLU function is a general activation function and is used in most cases these days\n",
    "- If we encounter a case of dead neurons in our networks, the leaky ReLU function is the best choice\n",
    "- The ReLU function should only be used in the hidden layers\n",
    "- As a rule of thumb, you can begin with using the ReLU function and then move over to other activation functions in case ReLU doesn’t provide optimum results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl model with more dense layers\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(n_features,), kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e741d56",
   "metadata": {},
   "source": [
    "#### REGULARIZATION Techniques\n",
    "\n",
    "- when the model performs better on train dataset but prediction result is poor on test dataset\n",
    "- regularization layers help to avoid overfitting and more efficient model\n",
    "- regularization techiques improves model performance and converge faster\n",
    "- regularization tools : early stopping, dropout, weight initialization techniques, and batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, kernel_initializer='he_normal', activation='relu', input_shape=(n_features,))))\n",
    "model.add(BatchNormalization())    \n",
    "model.add(Dense(30, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(10, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(1, kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279db3a1",
   "metadata": {},
   "source": [
    "##### Batch normalization process \n",
    "- Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. \n",
    "- faster training with better accuracy\n",
    "- smoothens the loss function, optimizes model parameters \n",
    "- appropriate way to add this layer before or after activation functions\n",
    "- should not be used along with Dropout layer\n",
    "\n",
    "- used in MLP, CNN, RNN architectures\n",
    "\n",
    "Read more :\n",
    "- https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/\n",
    "- https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "- https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6\n",
    "- https://www.kaggle.com/ryanholbrook/dropout-and-batch-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b34151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_features,))))\n",
    "model.add(BatchNormalization())    \n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b902971",
   "metadata": {},
   "source": [
    "#### Using Dropout and Batch Normalization together\n",
    "\n",
    "##### Red Wine model\n",
    "\n",
    "- Dropout to control overfitting\n",
    "- Batch normalization to speed up optimization\n",
    "- when adding Dropout with Batch normalization, in the dense layer there will be higher units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059bd44",
   "metadata": {},
   "source": [
    "##### Dropout \n",
    "\n",
    "The term dropout refers to dropping or removing neurons at random from a hidden layer in a neural network to avoid overfitting.\n",
    "\n",
    "**What should be the optimum value of a dropout in a hidden layer?**\n",
    "- During the training, the activation is randomly set to 0. Typically 50% of activations in layers are dropped. \n",
    "- Most of the time, the Value of 0.2 to 0.4 is the Most preferable for Dropout, which means deactivating 20 to 40% of the Neurons from the Hidden Layers.\n",
    "- Always Try to Maintain a Dropout Value lesser than 0.5, As Higher than 0.5 Value for Dropout can lead to Overfitting.\n",
    "- Dropout values Less than 0.2 value will have no or less Significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(n_features,)))\n",
    "model.add.Dropout(0.3)\n",
    "model.add(BatchNormalization())    \n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add.Dropout(0.3)\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add.Dropout(0.3)\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add.Dropout(0.3)\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cbd3fe",
   "metadata": {},
   "source": [
    "Read more.......... \n",
    "- Book:  https://b-ok.asia/book/12248872/86a2fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e67c39",
   "metadata": {},
   "source": [
    "#### Learning rates \n",
    "- Tuning parameter for Optimization Algorithms\n",
    "- Optimizers : Momentum, RMS Prop, Adagrad,  Adadelta,  Adam\n",
    "\n",
    "\n",
    "read more: \n",
    "- https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423957e7",
   "metadata": {},
   "source": [
    "#### Adam :\n",
    "Adam converges to a better loss, faster compared to the others. Adam Optimizer is quite tried and tested, and in many deep learning projects, Adam Optimizer is used mostly by practitioners. It is easy to implement, Computationally efficient, Little memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4449db06",
   "metadata": {},
   "source": [
    "**Gradient Descent Algorithms as Optimization techniques :**\n",
    "- Gradient Descent is also known as “Backpropagation” is an optimization technique that is used to improve deep learning and neural network-based models by minimizing the loss function.\n",
    "- In simple terms, Gradient Descent is used for training a deep neural network. \n",
    "- Two important terms here - 1) Minimization  2) Loss Function\n",
    "\n",
    "-**Loss Function:**\n",
    "--- 'mse'(mean squared error) is used for regression problems.\n",
    "\n",
    "The other most common loss functions: \n",
    "- 'binary_crossentropy'--- binary classification.\n",
    "- 'sparse_categorical_crossentropy' ---- multi-class classification.\n",
    "\n",
    "- Loss function quantifies the amount of error in your predictions, It is a metric that is directly related to the performance of the model.\n",
    "- If Predictions are Good, then the Loss function would output a Lower Number and if your predictions are totally off, the Loss function will output a higher number. \n",
    "- Our Loss Function should always justify two things while generating,\n",
    "- It should always be low or negative-oriented: it is always a better option\n",
    "- It should be differentiable: should be able to apply derivative on it\n",
    "\n",
    "-**Minimization:**\n",
    "- As loss function is negative-oriented, the neural network always strives for making it less, this process is called Minimization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate added in a simple MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim =2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad2219",
   "metadata": {},
   "source": [
    "**Batch gradient descent algorithm**\n",
    "We take the whole Training Data as a batch and pass it to the network. The network calculates the Gradient for the whole Training Data and Updates the weights. If we observe closely an epoch in batch gradient descent, we just update our weights once.\n",
    "- Advantage: \n",
    "- Speed\n",
    "As the whole data is passed as a batch, the network would process the data at once in a vectorized way. We all know modern GPUs love batch processing, so the speed at which we train the model is super high.\n",
    "- Disadvantage:\n",
    "- Poor Learning in Initial Phases:  As the weights are updated just once. For every epoch, there is a high chance of the model underfitting, given our problem statement is complex to solve.\n",
    "- High Memory Consumption: Fitting the whole data at once is impossible unless your data is too small. Imagine you want to train a model on data of size 1000GB. So while using batch gradient descent you would have to bring this 1000GB onto your GPU or RAM, which is impossible.\n",
    "- It is recommended to use batch gradient descent when the data set is small and the problem statement is not too complex.\n",
    "\n",
    "**Stochastic gradient descent algorithm (SGD)**\n",
    "Here we take each data point and calculate its gradient. Next, we update weights and repeat this process. So if our dataset has 1000 data points using stochastic gradient descent, we calculate gradients and update weight 1000 times in one epoch. \n",
    "- Advantage: Low memory consumption:  as we just load one data point at a time. There is no restriction on how big our data can be.\n",
    "- Disadvantage:  Low speed: Here we do not have any batch processing nor vectorization, which means it is very slow to train. \n",
    "- Over learning: As the weights are updating very frequently, there is a chance for our model to overfit over simple problem statements as well. \n",
    "- If the data set is small and the problem statement is complex, then try stochastic gradient descent.\n",
    "**Mini Batch Gradient Descent:**\n",
    " Here, we internally match the training data. We then calculate gradient and update weights for each batch. We repeat this process until gradients over the whole training data are calculated and weights are updated.\n",
    "So, if our dataset has 1000 examples and our batch size is supposed 100, then we update the weights 10 times for each epoch.\n",
    "- Advantages: \n",
    "Low memory consumption, high speed\n",
    "Better memory optimization compared to batch gradient descent\n",
    "due to many batches as weights are updated less frequently compared to\n",
    "Stochastic gradient descent. It has fewer chances of overfitting.\n",
    "If your dataset is large, then mini-batch gradient descent should be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efc048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "model.add(Dense(512, activation='relu', input_shape=(10,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "#Compile neural network\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy']) \n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='MNIST_pred', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "print(callbacks)\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(X_train, y_train, epochs=20, callbacks=callbacks, verbose=0, batch_size=100, validation_data=(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0570060",
   "metadata": {},
   "source": [
    "### Early Stopping Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65756e24",
   "metadata": {},
   "source": [
    "- Early stopping is basically stopping the training once you reached the minimum of your losses or errors.\n",
    "- When we use too many epochs it leads to overfitting, too less epochs leads to underfitting of the model.This method allows us to specify a large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde14008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=250, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3f2f5",
   "metadata": {},
   "source": [
    "##### Model Evaluation matrices : \n",
    "\n",
    "Read More: \n",
    "- https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/\n",
    "- https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c709b8",
   "metadata": {},
   "source": [
    "#### Batch size & Epochs\n",
    "Read more :\n",
    "- https://deeplizard.com/learn/video/U4WB9p6ODjM\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c26636",
   "metadata": {},
   "source": [
    "##### **Batch Size:** \n",
    "The selection of batch size depends on the RAM or GPU Memory. The more RAM we have, the higher Batch size can be fitted into Memory. \n",
    "\n",
    "**How to select the Batch Size in Neural Networks?**\n",
    "- Generally The Batch Size of 32 is Good, But we should also try the Batch Sizes of 64, 128, and 256.\n",
    "- If the problem statement is complex, having a low batch size works better as low batch size and mini-batch gradient descent we will have more weight updates which ensures our weights are better tuned. \n",
    "- If your problem statement is simple, then try to have the maximum batch size that can fit your memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3acc3",
   "metadata": {},
   "source": [
    "**Epochs**:\n",
    "- The number of epochs is a hyperparameter that defines the number of times that the learning algorithm will work through the entire training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
